# RAG Chatbot — README

This repository contains a **Retrieval-Augmented Generation (RAG) chatbot** that: ingests documents from `./archive/`, stores chunk embeddings in **Milvus**, retrieves relevant chunks at query time, and uses a local Hugging Face LLM pipeline (or compatible LLM) to generate grounded answers.

> **Note:** This README was generated by reading the repository code (`main.py`, `model.py`, `docker-compose.yml`, and `.env`). Use this file as the canonical `README.md` for the project.

---

## Quick summary (what this project does)

- Loads documents from the `./archive/` folder.
- Splits/loads documents using a LangChain `DirectoryLoader` and converts them into documents acceptable by LangChain.
- Creates / connects to a Milvus vector store via `langchain_community.vectorstores.Milvus` and inserts documents + embeddings (via `langchain_community.embeddings.HuggingFaceEmbeddings`).
- At runtime (CLI `main.py`) the `RAG` class:
  - embeds the user query,
  - retrieves top-k similar chunks from Milvus,
  - builds a prompt with context + chat history,
  - and generates an answer using a Hugging Face text-generation pipeline (or local LLM) wrapped as a LangChain LLM.
- Conversation history is maintained using LangChain's `ConversationTokenBufferMemory` so the assistant can use short-term context.

---

## Tech / libraries used

- Python 3.9+ (3.9/3.10/3.11 recommended)
- `pymilvus` / `milvus` (Milvus client)
- `langchain`, `langchain_core`, `langchain_community`
- `HuggingFaceEmbeddings` (langchain\_community)
- `HuggingFacePipeline` / `transformers` (LLM pipeline)
- `dotenv` (load `.env`)
- `ConversationTokenBufferMemory` (LangChain memory)
- Docker & Docker Compose (for Milvus + etcd + MinIO)

See `requirements.txt` for exact package lines included in this repo.

---

## Project layout (relevant files)

```
./
├─ .env                 # MILVUS_HOST & MILVUS_PORT (already present)
├─ docker-compose.yml   # Milvus + etcd + MinIO configuration (included)
├─ main.py              # CLI entrypoint that instantiates RAG and accepts questions
├─ model.py             # RAG class: ingestion, retrieval, LLM-inference, chat-history
├─ requirements.txt     # Python packages
├─ archive/             # Place your source files here (md/txt/csv sample included)
└─ README.md            # <-- this file (created/updated)
```

---

## Environment variables (`.env`)

The repository includes a minimal `.env` with:

```ini
MILVUS_HOST = "127.0.0.1"
MILVUS_PORT = "19530"
```

- If you run the Milvus stack locally with the included `docker-compose.yml` and call the Python scripts from the host machine, keep `MILVUS_HOST=127.0.0.1` and `MILVUS_PORT=19530`.
- If you run your Python code inside a Docker container on the same network as the Milvus services, set `MILVUS_HOST=milvus-standalone` (the service name) and `MILVUS_PORT=19530`.

---

## How to run (full, sequential steps)

> These steps assume a Linux/macOS host. For Windows, adjust shell commands accordingly.

1. **Clone repository and inspect files**

```bash
git clone <your-repo-url>
cd <your-repo>
```

2. **Start Milvus stack with Docker Compose**

The repo includes `docker-compose.yml`. Start services:

```bash
docker compose up -d
```

- The compose file maps gRPC port `19530` to the host.
- NOTE: the provided `docker-compose.yml` intentionally maps the Milvus REST/metrics port to host `9191:9091` to avoid conflicts with a commonly-used `9091` on hosts. If you need metrics on standard `9091`, change it but be careful with port conflicts.

Give Milvus \~20–30s to initialize (etcd & MinIO must be ready first).

3. **Create & activate virtualenv, install Python deps**

```bash
python -m venv .venv
. .venv/bin/activate       # Windows: .venv\Scripts\activate
pip install -r requirements.txt
```

4. **Verify **``

Make sure `.env` has the correct `MILVUS_HOST` and `MILVUS_PORT`. The included `.env` defaults to localhost.

5. **(Optional) Inspect / add your documents**

- The code reads documents from `./archive/` — there is a sample CSV included (`archive/rag_sample_qas_from_kis.csv`).
- Add your `.md`, `.txt`, or other plain text files into `archive/` if you want custom data.

6. **Run the application (CLI)**

The entrypoint is `main.py` which instantiates the `RAG` class defined in `model.py`.

```bash
python main.py
```

- You will see a prompt: `Question:`. Type a question and press enter.
- Type `exit` to quit.

7. **What **``** does**

- `main.py` creates `RAG(docs_dir="./archive", n_retrievals=2, chat_max_tokens=3097, creativeness=1.2)` and loops reading user input.
- The `RAG.ask(question)` method returns the generated answer.

---

## Important implementation details (read the code)

- **Loader & VectorStore**: `model.py` uses `langchain_community.document_loaders.DirectoryLoader` to load the files in `docs_dir` and `langchain_community.vectorstores.Milvus` to persist documents.
- **Embeddings**: `HuggingFaceEmbeddings` is the embedding backend — ensure your environment has access to Hugging Face models or a local model cache.
- **LLM**: The RAG class sets up an LLM via a Hugging Face `pipeline` (transformers) or `HuggingFacePipeline` wrapper. The code constructs a prompt using `ChatPromptTemplate` and runs the pipeline to generate answers.
- **Chat memory**: `ConversationTokenBufferMemory` is used to save & inject recent chat-history into prompts.
- **LangChain chaining**: The code composes a prompt and chains it with the LLM and an `StrOutputParser` to produce a textual answer.

---

## Sequence diagrams (paste into [https://sequencediagram.org/](https://sequencediagram.org/))

### User query flow (paste below into sequencediagram.org)

![alt text](<RAG Chatbot - User Query.png>)

### Ingestion flow

![alt text](<RAG Chatbot- Ingestion Flow.png>)

## Troubleshooting (specific to this repo)

``

1. `docker ps` — ensure the three containers are running: `milvus-etcd`, `milvus-minio`, `milvus-standalone`.
2. `docker logs milvus-standalone` — watch for errors about etcd/minio connectivity.
3. If `Bind for 0.0.0.0:9091 failed` was previously seen, note this repo maps `9191:9091` (see `docker-compose.yml`) to avoid the conflict. Ensure you don't have another service using `9191`.
4. If your Python code runs inside Docker (not on the host), set `MILVUS_HOST=milvus-standalone` (service name), not `127.0.0.1`.
5. Wait 20–30 seconds after `docker compose up -d` before running Python ingestion or queries.

**Hugging Face models / transformer errors**

- If the LLM or embedding model cannot download, ensure the host has internet access or you have the models cached locally.
- For large Hugging Face models (Falcon, LLaMA variants), you may need adequate GPU/CPU resources and the appropriate `transformers`/accelerate\` setup.

---

## Customization & common edits

- `main.py`: change `n_retrievals`, `chat_max_tokens`, `creativeness` when instantiating `RAG(...)`.
- `model.py`:
  - Change the LLM model name in `__set_llm_model`.
  - Change embeddings model used by `HuggingFaceEmbeddings` (if supported by your environment).
  - Adjust Milvus collection name / index params if you want custom behavior.



